\chapter{Experimenteller Aufbau}
\label{chap:experiment}

Als Kern dieser Arbeit werden sowohl die RNN-Architekturen gewöhnliche RNN, LSTM und GRU, als auch die klassischen Klassifikatoren kNN, SVM, MLP und DT implementiert, getestet, ausgewertet und miteinander verglichen. Das Experiment verwendet, um Vergleichbarkeit sicherzustellen, für alle Klassifikatoren denselben in Kapitel 3 beschriebenen \glqq 121 Forearm EMG\grqq{} Datensatz von \cite{Kaufmann2013Data}. Um eine möglichst hohe Generalisierbarkeit der erzielten Ergebnisse zu erreichen, wurden alle Architekturen mittels einer 10-fachen \textit{Cross Validation} evaluiert. Diese ist nach \cite{kohavi1995study} ein Vorgehen zum Sicherstellen der Generalisierbarkeit von Ergebnissen und geschieht durch die Aufteilung des Datensatzes in $k$ inetwa gleich große Abschnitte. Daraufhin wird der Klassifikator einmal mit jedem Abschnitt der Daten getestet und mit dem Rest trainiert. Daraus ergibt sich eine durchschnittliche Klassifikationsgenauigkeit aus allen $k$ Durchläufen und vermeidet dadurch, dass manche Klassifikatoren durch zufällig \glqq einfacher\grqq{} zu klassifizierende Daten besser abschneiden. Gleichzeitig bedeutet k-fache Cross Validation aber auch einen dementsprechend höheren rechnerischen Aufwand (\cite{kohavi1995study}). Alle Hyperparameter werden aufgrund der rechenintensiven Natur mancher Klassifikatoren manuell optimiert. Im Fall der RNN-Architekturen wurde eine Lernrate von 0,001 gewählt. Um Overfitting in den RNN-Architekturen zu Vermeiden, wurde in diesen die \textit{Dropout}-Methodik (\cite{srivastava2014dropout}) verwendet. Diese schließt während des Trainings zufällig einen gewissen Prozentteil der Neuronen, inklusive ihrer Verbindungen, aus (\cite{srivastava2014dropout}). Dadurch passen sich Neuronen weniger aufeinander an und das Potenzial für Overfitting wird nach (\cite{srivastava2014dropout}) reduziert (\cite{srivastava2014dropout}). Um \textit{internal Covariance Shift} zu vermeiden, der durch die Anpassung der Gewichte und die dadurch entstehenden Unterschiede in der Verteilung der Neuronen in DNN zu Problemen führen kann (\cite{ioffe2015batch}), wird für das Training der RNN Strukturen \textit{Batch Normalization} (\cite{ioffe2015batch}) verwendet. Die Konfiguration sowie das Training und die Auswertung der verschiedenen Klassifikatoren geschieht mithilfe der Keras API von Tensorflow und Scikit Learn in der Programmiersprache Python. Das Training erfolgt in der CPU-Version von Tensorflow auf einem Intel 2,3 GHz 8-Kern i9 Prozessor.

Primäre Metriken für den Vergleich der Klassifikatoren sind die Klassifikationsgenauigkeit sowie die Trainingsdauer. Zunächst werden die gewöhnlichen state-of-the-art Klassifikatoren kNN, SVM, MLP und DT miteinander verglichen und potenzielle Besonderheiten in der Auswertung dieser aufgeführt. Anschließend werden die RNN-Architekturen zunächst miteinander und anschließend mit den am besten abschneidenden state-of-the-art Klassifikator verglichen, wobei hier ebenfalls auf Besonderheiten in den Ergebnissen und der Auswertung hingewiesen wird. Die Vorstellung und Auswertung der Ergebnisse erfolgt in Kapitel 8.