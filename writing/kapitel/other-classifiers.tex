\chapter{Gewöhnliche Klassifikatoren}
\label{chap:other-classifiers}

Gegenstand dieser Arbeit ist der Vergleich einiger DNN, insbesondere im Bereich von RNN-Architekturen, mit anderen state-of-the-art Klassifikatoren. Namenhaft sind diese kNN, SVM, Entscheidungsbäume sowie als einziges verglichenes Neurales Netz ein MLP. In den folgenden Unterkapiteln werden die angesprochenen Klassifikatoren im Hinblick auf ihre Funktionsweise, Konfiguration und im Test mit dem Datensatz erzielten Ergebnissen dargestellt. Im folgenden Kapitel erfolgt anschließend die Auswertung und der Vergleich mit den behandelten RNN-Architekturen.


\section{k Nearest Neighbors}
\label{sec:knn}

kNN ist der simpelste in dieser Forschungsarbeit behandelte Algorithmus. Er beinhaltet nach \cite{Kaufmann2013} nur einen Parameter $k$, der für eine Klassifikation alle möglichen Werte durchlaufen muss, um den für den Datensatz besten Wert zu finden. Diese Eigenschaft sowie die zumeist im Vergleich zu anderen Klassifikatoren wie SVM, DT oder DNN geringere Klassifikationsgenauigkeit machen kNN zu einem Klassifikator, der in der Praxis selten Anwendung findet (\cite{Kaufmann2013}). Trotzdem wird er im Vergleich zu anderen Klassifikatoren gerne referenziert, so wie auch in dieser Arbeit. kNN benötigen nach \cite{Kaufmann2013} keine Trainingsphase. Es werden die $k$ nächsten Nachbarn des Wertes einer Klasse aus dem Trainingsdatensatz, gemessen nach Euklidischer Distanz, zum Setzen der Klassifikationsgrenze verwendet (\cite{Kaufmann2013}). Unter Verwendung dieser Abgrenzung wird anschließend der Validierungsdatensatz getestet und die Klassifikationsgenauigkeit bestimmt.

Mit einer Klassifikationsgenauigkeit von 82,92\% bei einem $k$ von 16 lieferte der kNN Algorithmus das für alle 17 durchlaufenen $k$s beste Ergebnis. Sowohl höhere, als auch niedrigere Werte für $k$ führen zu einer schlechteren Klassifikationsgenauigkeit.

\section{Support Vector Machines}
\label{sec:svm}

SVM verwenden nach \cite{duda2012pattern} sogenannte Kernel Funktionen, um systematisch Klassen in höheren Dimensionen, als der, in der die Daten vorliegen, zu separieren. \cite{duda2012pattern} spircht dabei von \glqq[der] Vorverarbeitung der Daten zum repräsentieren von Mustern in hohen Dimensionen\grqq{} \cite{duda2012pattern}. SCV sind dabei Hyperplanes mit einem sogenannten \textit{margin}, der den Astand zwischen den \textit{Support Vectros} (SV) und dem Hyperplane darstellt. Dabei gilt, dass die Klassifikationsgenauigkeit umso höher ist, je größer der Abstand zwischen den SV und dem Hyperplane ist. Diese einfache Variante der SVM bezeichnet man auch als \textit{Maximal Margin Classifier} (\cite{cristianini2000}). Diese sind in der Praxis nach \cite{cristianini2000} allerdings schlecht anwendbar, da sie keinen Klassifikationsfehler zulassen und somit entweder vorliegende Daten nicht klassifizieren können oder Overfitting eintritt. Aus diesem Grund wird ein sogenannter \textit{soft margin} (SM) verwendet, der anders als der in \textit{Maximal Margin Classifiers} verwendete \textit{margin}, Fehlklassifikationen zulässt, um auch in komplexen Anwendungen eine insgesamt höhere Klassifikationsgenauigkeit zu erreichen (\cite{cristianini2000}). Das entspricht dem Ansatz des \glqq Bias/Varianz Dilemmas\grqq{} in der Klassifikation von Daten nach \cite{friedman1997bias}, da hier durch Zulassen von Klassifikationsfehlern und somit einer geringeren Varianz, ein höherer Bias erzielt wird, der insgesamt zu einer besseren Klassifikationsgenauigkeit in Test-Datensätzen führt. SVM finden den optimalen Hyperplane rechnerisch effizient, ohne die Daten für den Vergleich vorerst in die jeweilige Dimension transformieren zu müssen (\cite{cristianini2000}). Dieses Vorgehen ist allgemein bekannt als der ''Kernel Trick''. Neben der rechnerischen Effizienz ist der größte Vorteil von SVM ihr unterliegendes Prinzip der Minimierung von Strukturiertem Risiko (\cite{Kaufmann2013}). Eine vergleichsweise gute Klassifikationsgenauigkeit des hier verwendeten Datensatzes ist somit zu erwarten, da MLP häufig gute Ergebnisse in der Generalisierung liefern (\cite{Kaufmann2013}).

Für diese Arbeit wurde die Lineare Kernel Funktion gewählt, da diese im Vergleich mit den anderen Kernel Funktionen mit 87,88\% die beste Klassifikationsgenauigkeit erzielte.

\section{Multi Layer Perceptron}
\label{sec:mlp}

MLP sind der einzige hier adressierte Klassifikator, der neben den RNN-Archtekturen aus der Domäne der Neuronalen Netze stammt. MLP besitzen mindestens zwei Schichten, also einen HL und eine Output-Schicht (\cite{jain1996artificial}). Sie nutzen für die Optimierung der Parameter BP, um durch Gradientenabstieg die Kostenfunktion des Netzwerkes zu minimieren (\cite{jain1996artificial}). Anders als Klassifizierer wie die SVM verfügen MLP nicht über Eigenschaften zur Minimierung von strukturiertem Risiko, weshalb mögliches Overfitting durch manuelles Testen und Anpassen der Hyperparameter, vermieden werden muss (\cite{Kaufmann2013}). Weiterhin muss die Struktur von MLP, genauso wie bei jedem Neuralen Netzwerk, über die Anzahl der Layer bishin zu Anzahl der Neuronen und der verwendeten Aktivierungsfunktionen händisch festgelegt und optimiert werden (\cite{haykin2007neural}).

Die im Rahmen dieser Forschung untersuchte MLP-Architektur wird, entsprechend der Anzahl an Merkmalen, durch einen Input Layer mit 16 Neuronen initialisiert. Anschließend werden die Daten an den HL mit 15 Neuronen und einer \textit{rectified linear unit} (ReLu) Aktivierungsfunktion initialisiert. Abschließend folgt der Output Layer mit elt, der Anzahl der Klassen entsprechenden, Neuronen und einer \textit{softmax} Aktivierungsfunktion, durch die eine der Klassen als Ergebnis des Durchlaufs ausgegeben wird. Das Modell verwendet den Adam Optimizer (\cite{kingma2014adam}) für die automatische Optimierung des \textit{Gradientenabstiegs} (GD) mit einer \textit{Lernrate} (LR) von 0.001 und einer \textit{Batch Size} (BZ) von 1054. Die Architektur wurde über 600 Epochen trainiert. Die Gewichte aller Layer wurden mit einer Gaußischen Normalverteilung, die Biases mit einer Konstante 0 initialisiert.

Die Architektur erreichte nach manueller Optimierung der Hyperparameter sowie der Struktur eine Klassifikationsgenauigkeit von 86,14\% nach einer Trainingsdauer von 3 Minuten und 42 Sekunden.

\section{Entscheidungsbäume}
\label{sec:decision-trees}

Nachfolgender Abschnitt beruft sich für die Beschreibung der Funktionsweise von Entscheidungsbäumen auf das Buch ''Pattern Classification'' von \cite{duda2012pattern}, welches sich mit verschiedenen Methoden der Klassifikation beschäftigt und den Aufbau von DT im Detail beschreibt. DT unterscheiden sich danach von den anderen hier untersuchten Klassifikatoren vor allem darin, dass anstatt der Verwendung von Vektoren und Zahlen zur Beschreibung und Klassifizierung von Mustern logisch angeordnete Listen von Eigenschaften verwendet werden. Sie benötigen daher für eine Klassifikation keine numerischen Daten, sondern können Muster auch anhand von mathematisch nicht greifbaren Konzepten wie beispielsweise Farben oder Geschmäckern unterscheiden. DT bilden eine baumartige Struktur aus sequentiellen ''ja/nein''- oder auch ''if/else''-Abfragen. Dabei ist die erste Abfrage, die sogenannte \textit{root node}, verbunden mit sukzessiven \textit{nodes}, die wiederum ''ja/nein''-Abfragen beinhalten (\cite{duda2012pattern}). Sogenannte \textit{leaf nodes} (LN), die keine anschließenden Verbindungen besitzen, stellen dabei eine Klassifizierungsentscheidung dar (\cite{Kaufmann2013}). Ein Vorteil dieser Architektur nach \cite{duda2012pattern} ist, dass die Klassifikationsentscheidungen als logische Regeln betrachtet und somit durch Menschen interpretiert werden können.

Der für diese Forschungsarbeit verwendete Algorithmus zur Erstellung des DT nutzt die Integration von Scikit Learn in der Programmiersprache Python, die auf eine optimierte Version des \textit{CART} Algorithmus zugreift (\cite{scikitDT}). Die Mindestanzahl von Instanzen pro LN wurde nach manueller Optimierung auf 2 festgelegt.

Mit dieser Konfiguration erzielten die Entscheidungsbäume eine 87,13\%ige Klassifikationsgenauigkeit auf dem Validierungsdatensatz in einer Trainingsdauer von etwa 15 Sekunden.